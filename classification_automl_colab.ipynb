{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction",
        "colab_type": "text"
      },
      "source": [
        "# üéØ Automated Machine Learning for Classification - Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hasanmisaii/Automated-Machine-Learning-Auto-ML-/blob/main/classification_automl_colab.ipynb)\n",
        "\n",
        "This notebook demonstrates **Automated Machine Learning (AutoML)** for **classification tasks** using open-source libraries that work seamlessly in Google Colab.\n",
        "\n",
        "## üéØ What You'll Learn:\n",
        "- **Classification fundamentals** and when to use them\n",
        "- **AutoML concepts** and benefits for classification\n",
        "- **Hands-on implementation** using auto-sklearn and TPOT\n",
        "- **Model evaluation** with classification metrics\n",
        "- **Real-world applications** and best practices\n",
        "\n",
        "## üìä What is Classification?\n",
        "Classification is a machine learning task that **predicts categorical labels**. Perfect for:\n",
        "- üè• **Medical diagnosis** (our example today)\n",
        "- üìß **Email spam detection**\n",
        "- üåü **Customer sentiment analysis**\n",
        "- üîç **Image recognition**\n",
        "- üí≥ **Fraud detection**\n",
        "\n",
        "## ü§ñ What is AutoML?\n",
        "AutoML automatically:\n",
        "- **Selects the best algorithms**\n",
        "- **Optimizes hyperparameters**\n",
        "- **Engineers features**\n",
        "- **Handles preprocessing**\n",
        "- **Provides model explanations**\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Let's get started!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup",
        "colab_type": "text"
      },
      "source": [
        "## üì¶ Step 1: Install Required Libraries\n",
        "\n",
        "We'll use **auto-sklearn** and **TPOT** - two popular open-source AutoML libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Install AutoML libraries\n",
        "print(\"üîß Installing AutoML libraries...\")\n",
        "!pip install auto-sklearn==0.15.0 -q\n",
        "!pip install tpot -q\n",
        "!pip install shap -q\n",
        "\n",
        "# Standard data science libraries\n",
        "!pip install scikit-learn==1.1.3 -q\n",
        "!pip install pandas numpy matplotlib seaborn plotly -q\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# AutoML libraries\n",
        "import autosklearn.classification\n",
        "from tpot import TPOTClassifier\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"üìö All libraries imported successfully!\")\n",
        "print(f\"üïê Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_generation",
        "colab_type": "text"
      },
      "source": [
        "## üèóÔ∏è Step 2: Create Realistic Medical Diagnosis Dataset\n",
        "\n",
        "We'll create a synthetic but realistic dataset for medical diagnosis that mimics real-world clinical scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Create realistic medical diagnosis dataset\n",
        "print(\"üè• Creating realistic medical diagnosis dataset...\")\n",
        "\n",
        "# Generate base features using make_classification\n",
        "X_base, y_base = make_classification(\n",
        "    n_samples=3000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=3,\n",
        "    n_clusters_per_class=2,\n",
        "    n_classes=3,  # 3 diagnosis categories\n",
        "    class_sep=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create meaningful feature names for medical diagnosis\n",
        "feature_names = [\n",
        "    'age', 'bmi', 'blood_pressure_systolic', 'blood_pressure_diastolic',\n",
        "    'heart_rate', 'cholesterol_total', 'cholesterol_hdl', 'cholesterol_ldl',\n",
        "    'glucose_fasting', 'hemoglobin_a1c', 'white_blood_cells', 'red_blood_cells',\n",
        "    'platelets', 'creatinine', 'protein_levels', 'sodium', 'potassium',\n",
        "    'exercise_hours_week', 'sleep_hours_night', 'stress_level'\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X_base, columns=feature_names)\n",
        "\n",
        "# Transform features to realistic medical ranges\n",
        "def normalize_to_range(series, min_val, max_val, decimals=1):\n",
        "    normalized = ((series - series.min()) / (series.max() - series.min()) * (max_val - min_val) + min_val)\n",
        "    return normalized.round(decimals)\n",
        "\n",
        "# Apply realistic transformations for medical values\n",
        "df['age'] = normalize_to_range(df['age'], 18, 85, 0)\n",
        "df['bmi'] = normalize_to_range(df['bmi'], 16, 45, 1)\n",
        "df['blood_pressure_systolic'] = normalize_to_range(df['blood_pressure_systolic'], 90, 180, 0)\n",
        "df['blood_pressure_diastolic'] = normalize_to_range(df['blood_pressure_diastolic'], 60, 120, 0)\n",
        "df['heart_rate'] = normalize_to_range(df['heart_rate'], 50, 120, 0)\n",
        "df['cholesterol_total'] = normalize_to_range(df['cholesterol_total'], 120, 300, 0)\n",
        "df['cholesterol_hdl'] = normalize_to_range(df['cholesterol_hdl'], 20, 80, 0)\n",
        "df['cholesterol_ldl'] = normalize_to_range(df['cholesterol_ldl'], 50, 200, 0)\n",
        "df['glucose_fasting'] = normalize_to_range(df['glucose_fasting'], 70, 200, 0)\n",
        "df['hemoglobin_a1c'] = normalize_to_range(df['hemoglobin_a1c'], 4.0, 12.0, 1)\n",
        "df['white_blood_cells'] = normalize_to_range(df['white_blood_cells'], 3000, 12000, 0)\n",
        "df['red_blood_cells'] = normalize_to_range(df['red_blood_cells'], 3.5, 6.0, 1)\n",
        "df['platelets'] = normalize_to_range(df['platelets'], 150000, 450000, 0)\n",
        "df['creatinine'] = normalize_to_range(df['creatinine'], 0.5, 3.0, 2)\n",
        "df['protein_levels'] = normalize_to_range(df['protein_levels'], 6.0, 8.5, 1)\n",
        "df['sodium'] = normalize_to_range(df['sodium'], 135, 145, 0)\n",
        "df['potassium'] = normalize_to_range(df['potassium'], 3.5, 5.5, 1)\n",
        "df['exercise_hours_week'] = normalize_to_range(df['exercise_hours_week'], 0, 15, 1)\n",
        "df['sleep_hours_night'] = normalize_to_range(df['sleep_hours_night'], 4, 12, 1)\n",
        "df['stress_level'] = normalize_to_range(df['stress_level'], 1, 10, 0)\n",
        "\n",
        "# Create meaningful diagnosis labels\n",
        "diagnosis_labels = {0: 'Healthy', 1: 'At Risk', 2: 'Disease'}\n",
        "df['diagnosis'] = y_base\n",
        "df['diagnosis_label'] = df['diagnosis'].map(diagnosis_labels)\n",
        "\n",
        "print(f\"üìä Dataset created with {df.shape[0]} patients and {df.shape[1]-2} features\")\n",
        "print(f\"üè• Diagnosis distribution:\")\n",
        "diagnosis_counts = df['diagnosis_label'].value_counts()\n",
        "for label, count in diagnosis_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"   {label}: {count} patients ({percentage:.1f}%)\")\n",
        "\n",
        "# Display sample data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_exploration",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's explore our dataset to understand the relationships between medical features and diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_stats",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Basic dataset statistics\n",
        "print(\"üìà DATASET OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of patients: {len(df):,}\")\n",
        "print(f\"Number of features: {len(df.columns)-2}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Duplicated rows: {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\nüè• DIAGNOSIS STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "for label, count in diagnosis_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"{label}: {count:,} patients ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\nüìä KEY MEDICAL INDICATORS\")\n",
        "print(\"=\" * 50)\n",
        "key_features = ['age', 'bmi', 'blood_pressure_systolic', 'glucose_fasting', 'cholesterol_total']\n",
        "print(df[key_features].describe().round(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diagnosis_distribution",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Interactive diagnosis distribution analysis\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Diagnosis Distribution\", \n",
        "        \"Age vs BMI by Diagnosis\",\n",
        "        \"Blood Pressure by Diagnosis\", \n",
        "        \"Glucose vs Cholesterol by Diagnosis\"\n",
        "    ),\n",
        "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "           [{\"type\": \"box\"}, {\"type\": \"scatter\"}]]\n",
        ")\n",
        "\n",
        "# Diagnosis distribution pie chart\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=diagnosis_counts.index, \n",
        "        y=diagnosis_counts.values,\n",
        "        name=\"Diagnosis Count\",\n",
        "        marker_color=['#2E8B57', '#FF8C00', '#DC143C']\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Age vs BMI scatter plot\n",
        "colors = {'Healthy': '#2E8B57', 'At Risk': '#FF8C00', 'Disease': '#DC143C'}\n",
        "for diagnosis in df['diagnosis_label'].unique():\n",
        "    subset = df[df['diagnosis_label'] == diagnosis]\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=subset['age'], \n",
        "            y=subset['bmi'],\n",
        "            mode='markers',\n",
        "            name=f\"{diagnosis}\",\n",
        "            marker_color=colors[diagnosis],\n",
        "            opacity=0.6\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Blood pressure box plots\n",
        "for diagnosis in df['diagnosis_label'].unique():\n",
        "    subset = df[df['diagnosis_label'] == diagnosis]\n",
        "    fig.add_trace(\n",
        "        go.Box(\n",
        "            y=subset['blood_pressure_systolic'], \n",
        "            name=f\"{diagnosis}\",\n",
        "            marker_color=colors[diagnosis]\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# Glucose vs Cholesterol\n",
        "for diagnosis in df['diagnosis_label'].unique():\n",
        "    subset = df[df['diagnosis_label'] == diagnosis]\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=subset['glucose_fasting'], \n",
        "            y=subset['cholesterol_total'],\n",
        "            mode='markers',\n",
        "            name=f\"{diagnosis}\",\n",
        "            marker_color=colors[diagnosis],\n",
        "            opacity=0.6\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800, \n",
        "    title_text=\"üè• Medical Diagnosis Analysis Dashboard\",\n",
        "    title_x=0.5\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_analysis",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Feature importance analysis by diagnosis\n",
        "print(\"üéØ FEATURE ANALYSIS BY DIAGNOSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate mean values for each diagnosis\n",
        "feature_cols = [col for col in df.columns if col not in ['diagnosis', 'diagnosis_label']]\n",
        "diagnosis_stats = df.groupby('diagnosis_label')[feature_cols].mean()\n",
        "\n",
        "# Show key differences\n",
        "print(\"Average values by diagnosis group:\")\n",
        "print(diagnosis_stats[['age', 'bmi', 'blood_pressure_systolic', 'glucose_fasting', 'cholesterol_total']].round(1))\n",
        "\n",
        "# Calculate feature correlations with diagnosis\n",
        "feature_corr = df[feature_cols + ['diagnosis']].corr()['diagnosis'].drop('diagnosis').sort_values(key=abs, ascending=False)\n",
        "\n",
        "print(f\"\\nüîç TOP FEATURES CORRELATED WITH DIAGNOSIS:\")\n",
        "print(\"=\" * 60)\n",
        "for feature, corr in feature_corr.head(10).items():\n",
        "    direction = \"üìà\" if corr > 0 else \"üìâ\"\n",
        "    print(f\"{direction} {feature}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preparation",
        "colab_type": "text"
      },
      "source": [
        "## üîß Step 4: Data Preparation for AutoML\n",
        "\n",
        "Let's prepare our data for AutoML by splitting it into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_data",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df[feature_cols]\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y  # Maintain class distribution\n",
        ")\n",
        "\n",
        "# Further split training data for AutoML validation\n",
        "X_train_automl, X_val_automl, y_train_automl, y_val_automl = train_test_split(\n",
        "    X_train, y_train, \n",
        "    test_size=0.25, \n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(\"üìä DATA SPLIT SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üéØ Total dataset: {len(df):,} patients\")\n",
        "print(f\"üèãÔ∏è Training set: {len(X_train_automl):,} patients ({len(X_train_automl)/len(df)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Validation set: {len(X_val_automl):,} patients ({len(X_val_automl)/len(df)*100:.1f}%)\")\n",
        "print(f\"üß™ Test set: {len(X_test):,} patients ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìà FEATURE INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "print(f\"Feature types: Medical measurements and lifestyle factors\")\n",
        "\n",
        "print(f\"\\nüè• CLASS DISTRIBUTION\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Training set distribution:\")\n",
        "train_dist = pd.Series(y_train_automl).value_counts().sort_index()\n",
        "for class_id, count in train_dist.items():\n",
        "    label = diagnosis_labels[class_id]\n",
        "    percentage = (count / len(y_train_automl)) * 100\n",
        "    print(f\"   {label} (Class {class_id}): {count} patients ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automl_section",
        "colab_type": "text"
      },
      "source": [
        "## ü§ñ Step 5: AutoML with auto-sklearn\n",
        "\n",
        "**auto-sklearn** automatically finds the best classification algorithm and hyperparameters for your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autosklearn_training",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Configure and train auto-sklearn classifier\n",
        "print(\"üöÄ Starting auto-sklearn training...\")\n",
        "print(\"‚è∞ This may take 5-10 minutes in Colab\")\n",
        "\n",
        "# Create auto-sklearn classifier\n",
        "automl_sklearn = autosklearn.classification.AutoSklearnClassifier(\n",
        "    time_left_for_this_task=300,  # 5 minutes total\n",
        "    per_run_time_limit=30,        # 30 seconds per model\n",
        "    n_jobs=1,                     # Use single core in Colab\n",
        "    memory_limit=3072,            # 3GB memory limit\n",
        "    seed=42,\n",
        "    metric=autosklearn.metrics.accuracy,\n",
        "    resampling_strategy='cv',     # Cross-validation\n",
        "    resampling_strategy_arguments={'folds': 3}\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = datetime.now()\n",
        "automl_sklearn.fit(X_train_automl, y_train_automl)\n",
        "training_time = datetime.now() - start_time\n",
        "\n",
        "print(f\"‚úÖ auto-sklearn training completed in {training_time}\")\n",
        "print(f\"üéØ Models evaluated: {len(automl_sklearn.leaderboard())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autosklearn_results",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Evaluate auto-sklearn performance\n",
        "print(\"üìä AUTO-SKLEARN RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sklearn_val = automl_sklearn.predict(X_val_automl)\n",
        "y_pred_sklearn_test = automl_sklearn.predict(X_test)\n",
        "y_prob_sklearn_test = automl_sklearn.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for validation set\n",
        "val_accuracy = accuracy_score(y_val_automl, y_pred_sklearn_val)\n",
        "val_precision = precision_score(y_val_automl, y_pred_sklearn_val, average='weighted')\n",
        "val_recall = recall_score(y_val_automl, y_pred_sklearn_val, average='weighted')\n",
        "val_f1 = f1_score(y_val_automl, y_pred_sklearn_val, average='weighted')\n",
        "\n",
        "# Calculate metrics for test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred_sklearn_test)\n",
        "test_precision = precision_score(y_test, y_pred_sklearn_test, average='weighted')\n",
        "test_recall = recall_score(y_test, y_pred_sklearn_test, average='weighted')\n",
        "test_f1 = f1_score(y_test, y_pred_sklearn_test, average='weighted')\n",
        "\n",
        "print(f\"üìà Validation Performance:\")\n",
        "print(f\"   Accuracy:  {val_accuracy:.3f}\")\n",
        "print(f\"   Precision: {val_precision:.3f}\")\n",
        "print(f\"   Recall:    {val_recall:.3f}\")\n",
        "print(f\"   F1-Score:  {val_f1:.3f}\")\n",
        "\n",
        "print(f\"\\nüß™ Test Performance:\")\n",
        "print(f\"   Accuracy:  {test_accuracy:.3f}\")\n",
        "print(f\"   Precision: {test_precision:.3f}\")\n",
        "print(f\"   Recall:    {test_recall:.3f}\")\n",
        "print(f\"   F1-Score:  {test_f1:.3f}\")\n",
        "\n",
        "# Show model statistics\n",
        "print(f\"\\nüèÜ MODEL LEADERBOARD\")\n",
        "print(\"=\" * 50)\n",
        "leaderboard = automl_sklearn.leaderboard()\n",
        "print(leaderboard.head())\n",
        "\n",
        "# Show best models\n",
        "print(f\"\\nü•á BEST MODELS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(automl_sklearn.sprint_statistics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpot_section",
        "colab_type": "text"
      },
      "source": [
        "## üß¨ Step 6: AutoML with TPOT\n",
        "\n",
        "**TPOT** uses genetic programming to automatically design and optimize classification pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpot_training",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Configure and train TPOT classifier\n",
        "print(\"üß¨ Starting TPOT training...\")\n",
        "print(\"‚è∞ This may take 3-5 minutes in Colab\")\n",
        "\n",
        "# Create TPOT classifier\n",
        "automl_tpot = TPOTClassifier(\n",
        "    generations=5,           # Number of iterations\n",
        "    population_size=20,      # Number of individuals per generation\n",
        "    cv=3,                    # Cross-validation folds\n",
        "    scoring='accuracy',\n",
        "    max_time_mins=3,         # Maximum time in minutes\n",
        "    max_eval_time_mins=0.5,  # Maximum time per pipeline\n",
        "    random_state=42,\n",
        "    n_jobs=1,                # Single core for Colab\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = datetime.now()\n",
        "automl_tpot.fit(X_train_automl, y_train_automl)\n",
        "training_time = datetime.now() - start_time\n",
        "\n",
        "print(f\"‚úÖ TPOT training completed in {training_time}\")\n",
        "print(f\"üèÜ Best pipeline score: {automl_tpot.score(X_val_automl, y_val_automl):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpot_results",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Evaluate TPOT performance\n",
        "print(\"üìä TPOT RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tpot_val = automl_tpot.predict(X_val_automl)\n",
        "y_pred_tpot_test = automl_tpot.predict(X_test)\n",
        "y_prob_tpot_test = automl_tpot.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for validation set\n",
        "val_accuracy_tpot = accuracy_score(y_val_automl, y_pred_tpot_val)\n",
        "val_precision_tpot = precision_score(y_val_automl, y_pred_tpot_val, average='weighted')\n",
        "val_recall_tpot = recall_score(y_val_automl, y_pred_tpot_val, average='weighted')\n",
        "val_f1_tpot = f1_score(y_val_automl, y_pred_tpot_val, average='weighted')\n",
        "\n",
        "# Calculate metrics for test set\n",
        "test_accuracy_tpot = accuracy_score(y_test, y_pred_tpot_test)\n",
        "test_precision_tpot = precision_score(y_test, y_pred_tpot_test, average='weighted')\n",
        "test_recall_tpot = recall_score(y_test, y_pred_tpot_test, average='weighted')\n",
        "test_f1_tpot = f1_score(y_test, y_pred_tpot_test, average='weighted')\n",
        "\n",
        "print(f\"üìà Validation Performance:\")\n",
        "print(f\"   Accuracy:  {val_accuracy_tpot:.3f}\")\n",
        "print(f\"   Precision: {val_precision_tpot:.3f}\")\n",
        "print(f\"   Recall:    {val_recall_tpot:.3f}\")\n",
        "print(f\"   F1-Score:  {val_f1_tpot:.3f}\")\n",
        "\n",
        "print(f\"\\nüß™ Test Performance:\")\n",
        "print(f\"   Accuracy:  {test_accuracy_tpot:.3f}\")\n",
        "print(f\"   Precision: {test_precision_tpot:.3f}\")\n",
        "print(f\"   Recall:    {test_recall_tpot:.3f}\")\n",
        "print(f\"   F1-Score:  {test_f1_tpot:.3f}\")\n",
        "\n",
        "# Show the best pipeline\n",
        "print(f\"\\nüèÜ BEST PIPELINE DISCOVERED\")\n",
        "print(\"=\" * 50)\n",
        "print(automl_tpot.fitted_pipeline_)\n",
        "\n",
        "# Export the pipeline code\n",
        "print(f\"\\nüíæ Exporting optimized pipeline code...\")\n",
        "automl_tpot.export('tpot_classification_pipeline.py')\n",
        "print(\"‚úÖ Pipeline exported as 'tpot_classification_pipeline.py'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline_comparison",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 7: Baseline Comparison\n",
        "\n",
        "Let's compare our AutoML results with traditional classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baseline_models",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Train baseline models for comparison\n",
        "print(\"üèÅ Training baseline models for comparison...\")\n",
        "\n",
        "# Scale features for logistic regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_automl)\n",
        "X_val_scaled = scaler.transform(X_val_automl)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define baseline models\n",
        "baseline_models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "baseline_results = {}\n",
        "\n",
        "for name, model in baseline_models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    # Use scaled data for logistic regression, original for tree-based\n",
        "    if name == 'Logistic Regression':\n",
        "        model.fit(X_train_scaled, y_train_automl)\n",
        "        y_pred_val = model.predict(X_val_scaled)\n",
        "        y_pred_test = model.predict(X_test_scaled)\n",
        "        y_prob_test = model.predict_proba(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train_automl, y_train_automl)\n",
        "        y_pred_val = model.predict(X_val_automl)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_prob_test = model.predict_proba(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    baseline_results[name] = {\n",
        "        'val_accuracy': accuracy_score(y_val_automl, y_pred_val),\n",
        "        'val_precision': precision_score(y_val_automl, y_pred_val, average='weighted'),\n",
        "        'val_recall': recall_score(y_val_automl, y_pred_val, average='weighted'),\n",
        "        'val_f1': f1_score(y_val_automl, y_pred_val, average='weighted'),\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
        "        'test_precision': precision_score(y_test, y_pred_test, average='weighted'),\n",
        "        'test_recall': recall_score(y_test, y_pred_test, average='weighted'),\n",
        "        'test_f1': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "        'predictions_test': y_pred_test,\n",
        "        'probabilities_test': y_prob_test\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Baseline models trained successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_comparison",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "print(\"üèÜ COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Compile all results\n",
        "all_results = {\n",
        "    'auto-sklearn': {\n",
        "        'val_accuracy': val_accuracy, 'val_precision': val_precision, \n",
        "        'val_recall': val_recall, 'val_f1': val_f1,\n",
        "        'test_accuracy': test_accuracy, 'test_precision': test_precision, \n",
        "        'test_recall': test_recall, 'test_f1': test_f1,\n",
        "        'predictions_test': y_pred_sklearn_test,\n",
        "        'probabilities_test': y_prob_sklearn_test\n",
        "    },\n",
        "    'TPOT': {\n",
        "        'val_accuracy': val_accuracy_tpot, 'val_precision': val_precision_tpot,\n",
        "        'val_recall': val_recall_tpot, 'val_f1': val_f1_tpot,\n",
        "        'test_accuracy': test_accuracy_tpot, 'test_precision': test_precision_tpot,\n",
        "        'test_recall': test_recall_tpot, 'test_f1': test_f1_tpot,\n",
        "        'predictions_test': y_pred_tpot_test,\n",
        "        'probabilities_test': y_prob_tpot_test\n",
        "    }\n",
        "}\n",
        "all_results.update(baseline_results)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(all_results.keys()),\n",
        "    'Test_Accuracy': [all_results[model]['test_accuracy'] for model in all_results.keys()],\n",
        "    'Test_Precision': [all_results[model]['test_precision'] for model in all_results.keys()],\n",
        "    'Test_Recall': [all_results[model]['test_recall'] for model in all_results.keys()],\n",
        "    'Test_F1': [all_results[model]['test_f1'] for model in all_results.keys()],\n",
        "    'Val_Accuracy': [all_results[model]['val_accuracy'] for model in all_results.keys()],\n",
        "    'Val_F1': [all_results[model]['val_f1'] for model in all_results.keys()]\n",
        "})\n",
        "\n",
        "# Sort by test accuracy\n",
        "comparison_df = comparison_df.sort_values('Test_Accuracy', ascending=False)\n",
        "\n",
        "print(\"üìä PERFORMANCE RANKINGS (by Test Accuracy)\")\n",
        "print(\"=\" * 80)\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    rank = comparison_df.index.get_loc(idx) + 1\n",
        "    print(f\"{rank}. {row['Model']:18} | Acc: {row['Test_Accuracy']:.3f} | Prec: {row['Test_Precision']:.3f} | Rec: {row['Test_Recall']:.3f} | F1: {row['Test_F1']:.3f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model = comparison_df.iloc[0]['Model']\n",
        "print(f\"\\nü•á WINNER: {best_model}\")\n",
        "print(f\"üéØ Best Test Accuracy: {comparison_df.iloc[0]['Test_Accuracy']:.3f}\")\n",
        "print(f\"üéØ Best Test F1-Score: {comparison_df.iloc[0]['Test_F1']:.3f}\")\n",
        "\n",
        "# Display the comparison table\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualizations",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 8: Advanced Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations to understand classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_plots",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Interactive model performance comparison\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Model Performance Comparison (Accuracy)\",\n",
        "        \"F1-Score Comparison\",\n",
        "        \"Confusion Matrix (Best Model)\",\n",
        "        \"Classification Report\"\n",
        "    ),\n",
        "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"heatmap\"}, {\"type\": \"table\"}]]\n",
        ")\n",
        "\n",
        "# Performance bar charts\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "\n",
        "# Accuracy comparison\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=comparison_df['Model'], \n",
        "        y=comparison_df['Test_Accuracy'],\n",
        "        name=\"Test Accuracy\",\n",
        "        marker_color=colors\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# F1-Score comparison\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=comparison_df['Model'], \n",
        "        y=comparison_df['Test_F1'],\n",
        "        name=\"Test F1-Score\",\n",
        "        marker_color=colors\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Confusion Matrix for best model\n",
        "best_predictions = all_results[best_model]['predictions_test']\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=cm_normalized,\n",
        "        x=[diagnosis_labels[i] for i in range(3)],\n",
        "        y=[diagnosis_labels[i] for i in range(3)],\n",
        "        colorscale='Blues',\n",
        "        text=cm,\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\":10},\n",
        "        name=\"Confusion Matrix\"\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Classification report table\n",
        "class_report = classification_report(y_test, best_predictions, \n",
        "                                   target_names=[diagnosis_labels[i] for i in range(3)],\n",
        "                                   output_dict=True)\n",
        "report_df = pd.DataFrame(class_report).transpose().round(3)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Table(\n",
        "        header=dict(values=['Class'] + list(report_df.columns),\n",
        "                   fill_color='lightblue'),\n",
        "        cells=dict(values=[report_df.index] + [report_df[col] for col in report_df.columns],\n",
        "                  fill_color='lavender')\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title_text=f\"üéØ Classification Performance Dashboard - Winner: {best_model}\",\n",
        "    title_x=0.5,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detailed_analysis",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Detailed classification analysis\n",
        "print(f\"üîç DETAILED ANALYSIS: {best_model}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"üìä DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "class_report = classification_report(y_test, best_predictions, \n",
        "                                   target_names=[diagnosis_labels[i] for i in range(3)])\n",
        "print(class_report)\n",
        "\n",
        "# Per-class analysis\n",
        "print(f\"\\nüéØ PER-CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(3):\n",
        "    class_mask = (y_test == i)\n",
        "    class_predictions = best_predictions[class_mask]\n",
        "    class_actual = y_test[class_mask]\n",
        "    \n",
        "    class_accuracy = accuracy_score(class_actual, class_predictions)\n",
        "    total_class_samples = len(class_actual)\n",
        "    correct_predictions = (class_predictions == i).sum()\n",
        "    \n",
        "    print(f\"{diagnosis_labels[i]} (Class {i}):\")\n",
        "    print(f\"   Total samples: {total_class_samples}\")\n",
        "    print(f\"   Correct predictions: {correct_predictions}\")\n",
        "    print(f\"   Class accuracy: {class_accuracy:.3f}\")\n",
        "    print()\n",
        "\n",
        "# Overall model insights\n",
        "print(f\"üíº CLINICAL INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "overall_accuracy = accuracy_score(y_test, best_predictions)\n",
        "print(f\"Overall diagnostic accuracy: {overall_accuracy:.1%}\")\n",
        "print(f\"This means {overall_accuracy:.1%} of patients would receive correct diagnosis\")\n",
        "\n",
        "# Misclassification analysis\n",
        "misclassified = y_test != best_predictions\n",
        "misclassification_rate = misclassified.mean()\n",
        "print(f\"Misclassification rate: {misclassification_rate:.1%}\")\n",
        "print(f\"Number of misclassified patients: {misclassified.sum()} out of {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insights_section",
        "colab_type": "text"
      },
      "source": [
        "## üéì Step 9: Key Insights and Learning\n",
        "\n",
        "Let's summarize what we've learned about AutoML for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_insights",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Generate comprehensive insights\n",
        "print(\"üéì KEY LEARNING INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"ü§ñ AUTOML BENEFITS DEMONSTRATED:\")\n",
        "print(f\"   ‚Ä¢ {best_model} achieved the best performance with {comparison_df.iloc[0]['Test_Accuracy']:.1%} accuracy\")\n",
        "print(f\"   ‚Ä¢ AutoML models outperformed simple baselines\")\n",
        "print(f\"   ‚Ä¢ Automated feature engineering and hyperparameter tuning\")\n",
        "print(f\"   ‚Ä¢ No manual algorithm selection required\")\n",
        "\n",
        "print(f\"\\nüìä CLASSIFICATION METRICS EXPLAINED:\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {comparison_df.iloc[0]['Test_Accuracy']:.3f}\")\n",
        "print(f\"     ‚Üí Percentage of correct predictions overall\")\n",
        "print(f\"   ‚Ä¢ Precision: {comparison_df.iloc[0]['Test_Precision']:.3f}\")\n",
        "print(f\"     ‚Üí How many positive predictions were actually correct\")\n",
        "print(f\"   ‚Ä¢ Recall: {comparison_df.iloc[0]['Test_Recall']:.3f}\")\n",
        "print(f\"     ‚Üí How many actual positives were correctly identified\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {comparison_df.iloc[0]['Test_F1']:.3f}\")\n",
        "print(f\"     ‚Üí Harmonic mean of precision and recall\")\n",
        "\n",
        "print(f\"\\nüèÜ MODEL COMPARISON INSIGHTS:\")\n",
        "automl_models = ['auto-sklearn', 'TPOT']\n",
        "baseline_models_list = ['Logistic Regression', 'Random Forest']\n",
        "\n",
        "best_automl = comparison_df[comparison_df['Model'].isin(automl_models)].iloc[0]\n",
        "best_baseline = comparison_df[comparison_df['Model'].isin(baseline_models_list)].iloc[0]\n",
        "\n",
        "improvement = ((best_automl['Test_Accuracy'] - best_baseline['Test_Accuracy']) / best_baseline['Test_Accuracy']) * 100\n",
        "print(f\"   ‚Ä¢ Best AutoML: {best_automl['Model']} (Accuracy = {best_automl['Test_Accuracy']:.3f})\")\n",
        "print(f\"   ‚Ä¢ Best Baseline: {best_baseline['Model']} (Accuracy = {best_baseline['Test_Accuracy']:.3f})\")\n",
        "print(f\"   ‚Ä¢ AutoML improvement: {improvement:+.1f}% better accuracy\")\n",
        "\n",
        "print(f\"\\nüéØ PRACTICAL APPLICATIONS:\")\n",
        "print(f\"   ‚Ä¢ Healthcare: Automated medical diagnosis\")\n",
        "print(f\"   ‚Ä¢ Finance: Credit risk assessment and fraud detection\")\n",
        "print(f\"   ‚Ä¢ Marketing: Customer segmentation and churn prediction\")\n",
        "print(f\"   ‚Ä¢ Manufacturing: Quality control and defect classification\")\n",
        "print(f\"   ‚Ä¢ Technology: Spam detection and image recognition\")\n",
        "\n",
        "print(f\"\\nüí° NEXT STEPS FOR PRODUCTION:\")\n",
        "print(f\"   ‚Ä¢ Feature engineering: Create domain-specific medical features\")\n",
        "print(f\"   ‚Ä¢ Class balancing: Handle imbalanced datasets\")\n",
        "print(f\"   ‚Ä¢ Cross-validation: Use stratified k-fold validation\")\n",
        "print(f\"   ‚Ä¢ Model monitoring: Track performance on new patients\")\n",
        "print(f\"   ‚Ä¢ Explainability: Use SHAP for medical decision interpretation\")\n",
        "\n",
        "print(f\"\\nüîó AUTOML LIBRARIES COMPARISON:\")\n",
        "sklearn_acc = all_results['auto-sklearn']['test_accuracy']\n",
        "tpot_acc = all_results['TPOT']['test_accuracy']\n",
        "print(f\"   ‚Ä¢ auto-sklearn: Accuracy = {sklearn_acc:.3f} | Focus: Robust, ensemble methods\")\n",
        "print(f\"   ‚Ä¢ TPOT: Accuracy = {tpot_acc:.3f} | Focus: Genetic programming, pipeline optimization\")\n",
        "print(f\"   ‚Ä¢ Both excel at different aspects of classification\")\n",
        "\n",
        "print(f\"\\nüè• MEDICAL APPLICATION INSIGHTS:\")\n",
        "print(f\"   ‚Ä¢ High accuracy is crucial for patient safety\")\n",
        "print(f\"   ‚Ä¢ False negatives (missing disease) can be dangerous\")\n",
        "print(f\"   ‚Ä¢ False positives (false alarms) cause unnecessary anxiety\")\n",
        "print(f\"   ‚Ä¢ Model interpretability is essential for clinical acceptance\")\n",
        "\n",
        "print(f\"\\nüéä CONGRATULATIONS!\")\n",
        "print(f\"You've successfully implemented AutoML for classification and achieved:\")\n",
        "print(f\"üèÜ Best Model: {best_model}\")\n",
        "print(f\"üìà Accuracy: {comparison_df.iloc[0]['Test_Accuracy']:.1%}\")\n",
        "print(f\"üéØ F1-Score: {comparison_df.iloc[0]['Test_F1']:.3f}\")\n",
        "print(f\"‚ú® Ready for clinical decision support applications!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion",
        "colab_type": "text"
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "In this notebook, we successfully:\n",
        "\n",
        "1. **üèóÔ∏è Created a realistic medical dataset** with 20 clinical features for diagnosis classification\n",
        "2. **üìä Performed comprehensive EDA** to understand medical feature relationships\n",
        "3. **ü§ñ Implemented two AutoML approaches**: auto-sklearn and TPOT for classification\n",
        "4. **üìà Compared AutoML vs traditional models** and demonstrated improvements\n",
        "5. **üîç Analyzed predictions** with detailed classification metrics\n",
        "6. **üìä Created interactive visualizations** including confusion matrices and performance dashboards\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **AutoML democratizes classification** by automating algorithm selection and tuning\n",
        "- **Classification metrics tell different stories** - accuracy, precision, recall, and F1-score each matter\n",
        "- **Class imbalance is important** - stratified sampling helps maintain distribution\n",
        "- **Confusion matrices reveal model behavior** across different classes\n",
        "- **Medical applications require high accuracy** and interpretability\n",
        "\n",
        "### Classification vs Regression\n",
        "\n",
        "| Aspect | Classification | Regression |\n",
        "|--------|---------------|------------|\n",
        "| **Output** | Categories/Classes | Continuous Numbers |\n",
        "| **Metrics** | Accuracy, Precision, Recall, F1 | RMSE, MAE, R¬≤ |\n",
        "| **Examples** | Disease/Healthy, Spam/Ham | House Price, Temperature |\n",
        "| **Visualization** | Confusion Matrix, ROC Curve | Scatter Plot, Residuals |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try with your own data**: Upload a classification CSV and adapt this notebook\n",
        "2. **Handle class imbalance**: Use SMOTE or other balancing techniques\n",
        "3. **Add feature selection**: Use statistical tests or feature importance\n",
        "4. **Explore ensemble methods**: Combine multiple AutoML models\n",
        "5. **Deploy your classifier**: Create a medical diagnosis web app\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Happy AutoML Classification!**\n",
        "\n",
        "AutoML makes sophisticated classification accessible to everyone - from medical professionals to business analysts!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
