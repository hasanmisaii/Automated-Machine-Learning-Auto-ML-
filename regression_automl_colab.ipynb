{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction",
        "colab_type": "text"
      },
      "source": [
        "# üè† Automated Machine Learning for Regression - Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hasanmisaii/Automated-Machine-Learning-Auto-ML-/blob/main/regression_automl_colab.ipynb)\n",
        "\n",
        "This notebook demonstrates **Automated Machine Learning (AutoML)** for **regression tasks** using open-source libraries that work seamlessly in Google Colab.\n",
        "\n",
        "## üéØ What You'll Learn:\n",
        "- **Regression fundamentals** and when to use them\n",
        "- **AutoML concepts** and benefits\n",
        "- **Hands-on implementation** using auto-sklearn and TPOT\n",
        "- **Model evaluation** and interpretation\n",
        "- **Real-world applications** and best practices\n",
        "\n",
        "## üìä What is Regression?\n",
        "Regression is a machine learning task that **predicts continuous numerical values**. Perfect for:\n",
        "- üè† **House price prediction** (our example today)\n",
        "- üìà **Stock price forecasting**\n",
        "- üå°Ô∏è **Temperature prediction**\n",
        "- üí∞ **Sales revenue estimation**\n",
        "- ‚ö° **Energy consumption forecasting**\n",
        "\n",
        "## ü§ñ What is AutoML?\n",
        "AutoML automatically:\n",
        "- **Selects the best algorithms**\n",
        "- **Optimizes hyperparameters**\n",
        "- **Engineers features**\n",
        "- **Handles preprocessing**\n",
        "- **Provides model explanations**\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Let's get started!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup",
        "colab_type": "text"
      },
      "source": [
        "## üì¶ Step 1: Install Required Libraries\n",
        "\n",
        "We'll use **auto-sklearn** and **TPOT** - two popular open-source AutoML libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Install AutoML libraries\n",
        "print(\"üîß Installing AutoML libraries...\")\n",
        "!pip install auto-sklearn==0.15.0 -q\n",
        "!pip install tpot -q\n",
        "!pip install shap -q\n",
        "\n",
        "# Standard data science libraries\n",
        "!pip install scikit-learn==1.1.3 -q\n",
        "!pip install pandas numpy matplotlib seaborn plotly -q\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# AutoML libraries\n",
        "import autosklearn.regression\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"üìö All libraries imported successfully!\")\n",
        "print(f\"üïê Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_generation",
        "colab_type": "text"
      },
      "source": [
        "## üèóÔ∏è Step 2: Create Realistic House Price Dataset\n",
        "\n",
        "We'll create a synthetic but realistic dataset for house price prediction that mimics real-world scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Create realistic house price dataset\n",
        "print(\"üè† Creating realistic house price dataset...\")\n",
        "\n",
        "# Generate base features using make_regression\n",
        "X_base, y_base = make_regression(\n",
        "    n_samples=2000,\n",
        "    n_features=15,\n",
        "    n_informative=12,\n",
        "    noise=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create meaningful feature names\n",
        "feature_names = [\n",
        "    'square_footage', 'bedrooms', 'bathrooms', 'age_years', 'lot_size_acres',\n",
        "    'garage_spaces', 'neighborhood_score', 'school_rating', 'crime_rate', \n",
        "    'distance_to_downtown_miles', 'property_tax_rate', 'walkability_score',\n",
        "    'num_floors', 'fireplace_count', 'pool_present'\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X_base, columns=feature_names)\n",
        "\n",
        "# Transform features to realistic ranges\n",
        "def normalize_to_range(series, min_val, max_val):\n",
        "    return ((series - series.min()) / (series.max() - series.min()) * (max_val - min_val) + min_val)\n",
        "\n",
        "# Apply realistic transformations\n",
        "df['square_footage'] = normalize_to_range(df['square_footage'], 800, 4000).round(0)\n",
        "df['bedrooms'] = normalize_to_range(df['bedrooms'], 1, 6).round(0)\n",
        "df['bathrooms'] = normalize_to_range(df['bathrooms'], 1, 4).round(1)\n",
        "df['age_years'] = normalize_to_range(df['age_years'], 0, 100).round(0)\n",
        "df['lot_size_acres'] = normalize_to_range(df['lot_size_acres'], 0.1, 2.0).round(2)\n",
        "df['garage_spaces'] = normalize_to_range(df['garage_spaces'], 0, 3).round(0)\n",
        "df['neighborhood_score'] = normalize_to_range(df['neighborhood_score'], 1, 10).round(1)\n",
        "df['school_rating'] = normalize_to_range(df['school_rating'], 1, 10).round(1)\n",
        "df['crime_rate'] = normalize_to_range(df['crime_rate'], 0.1, 5.0).round(2)\n",
        "df['distance_to_downtown_miles'] = normalize_to_range(df['distance_to_downtown_miles'], 0.5, 50).round(1)\n",
        "df['property_tax_rate'] = normalize_to_range(df['property_tax_rate'], 0.5, 3.0).round(3)\n",
        "df['walkability_score'] = normalize_to_range(df['walkability_score'], 1, 100).round(0)\n",
        "df['num_floors'] = normalize_to_range(df['num_floors'], 1, 3).round(0)\n",
        "df['fireplace_count'] = normalize_to_range(df['fireplace_count'], 0, 3).round(0)\n",
        "df['pool_present'] = (normalize_to_range(df['pool_present'], 0, 1) > 0.7).astype(int)\n",
        "\n",
        "# Create realistic target variable (house prices in thousands)\n",
        "# Base price calculation with realistic factors\n",
        "base_price = (\n",
        "    df['square_footage'] * 150 +  # $150 per sq ft\n",
        "    df['bedrooms'] * 10000 +      # $10k per bedroom\n",
        "    df['bathrooms'] * 8000 +      # $8k per bathroom\n",
        "    df['garage_spaces'] * 5000 +  # $5k per garage space\n",
        "    df['neighborhood_score'] * 15000 +  # Neighborhood premium\n",
        "    df['pool_present'] * 25000 +  # Pool adds $25k\n",
        "    df['fireplace_count'] * 3000  # $3k per fireplace\n",
        ")\n",
        "\n",
        "# Apply negative factors\n",
        "price_adjustments = (\n",
        "    - df['age_years'] * 500 +     # Depreciation\n",
        "    - df['crime_rate'] * 8000 +   # Crime reduces value\n",
        "    - df['distance_to_downtown_miles'] * 1000  # Distance penalty\n",
        ")\n",
        "\n",
        "# Final price with some noise\n",
        "y_realistic = (base_price + price_adjustments + np.random.normal(0, 15000, len(df))) / 1000\n",
        "y_realistic = np.maximum(y_realistic, 50)  # Minimum $50k\n",
        "df['price_thousands'] = y_realistic.round(1)\n",
        "\n",
        "print(f\"üìä Dataset created with {df.shape[0]} houses and {df.shape[1]-1} features\")\n",
        "print(f\"üí∞ Price range: ${df['price_thousands'].min():.0f}k - ${df['price_thousands'].max():.0f}k\")\n",
        "print(f\"üìç Average price: ${df['price_thousands'].mean():.0f}k\")\n",
        "\n",
        "# Display sample data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_exploration",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's explore our dataset to understand the relationships between features and house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_stats",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Basic dataset statistics\n",
        "print(\"üìà DATASET OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of houses: {len(df):,}\")\n",
        "print(f\"Number of features: {len(df.columns)-1}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Duplicated rows: {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\nüí∞ PRICE STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "price_stats = df['price_thousands'].describe()\n",
        "for stat, value in price_stats.items():\n",
        "    print(f\"{stat.capitalize()}: ${value:.1f}k\")\n",
        "\n",
        "# Display data types\n",
        "print(\"\\nüîß FEATURE TYPES\")\n",
        "print(\"=\" * 50)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_analysis",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Interactive correlation analysis with Plotly\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "price_correlations = correlation_matrix['price_thousands'].drop('price_thousands').sort_values(key=abs, ascending=False)\n",
        "\n",
        "# Create interactive correlation heatmap\n",
        "fig = px.imshow(\n",
        "    correlation_matrix,\n",
        "    text_auto=True,\n",
        "    aspect=\"auto\",\n",
        "    title=\"üî• Feature Correlation Heatmap\",\n",
        "    color_continuous_scale=\"RdBu_r\",\n",
        "    width=800,\n",
        "    height=700\n",
        ")\n",
        "fig.update_layout(title_x=0.5)\n",
        "fig.show()\n",
        "\n",
        "# Top correlations with price\n",
        "print(\"üéØ TOP FEATURES CORRELATED WITH PRICE\")\n",
        "print(\"=\" * 50)\n",
        "for feature, corr in price_correlations.head(8).items():\n",
        "    direction = \"üìà\" if corr > 0 else \"üìâ\"\n",
        "    print(f\"{direction} {feature}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "price_distribution",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Interactive price distribution analysis\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Price Distribution\", \n",
        "        \"Price vs Square Footage\",\n",
        "        \"Price vs Neighborhood Score\", \n",
        "        \"Price by Number of Bedrooms\"\n",
        "    ),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# Price histogram\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['price_thousands'], nbinsx=50, name=\"Price Distribution\"),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Scatter: Price vs Square Footage\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=df['square_footage'], \n",
        "        y=df['price_thousands'], \n",
        "        mode='markers',\n",
        "        name=\"Price vs Sq Ft\",\n",
        "        opacity=0.6\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Scatter: Price vs Neighborhood Score\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=df['neighborhood_score'], \n",
        "        y=df['price_thousands'], \n",
        "        mode='markers',\n",
        "        name=\"Price vs Neighborhood\",\n",
        "        opacity=0.6\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Box plot: Price by Bedrooms\n",
        "for bedroom_count in sorted(df['bedrooms'].unique()):\n",
        "    bedroom_data = df[df['bedrooms'] == bedroom_count]['price_thousands']\n",
        "    fig.add_trace(\n",
        "        go.Box(y=bedroom_data, name=f\"{int(bedroom_count)} BR\"),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800, \n",
        "    title_text=\"üìä House Price Analysis Dashboard\",\n",
        "    title_x=0.5\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preparation",
        "colab_type": "text"
      },
      "source": [
        "## üîß Step 4: Data Preparation for AutoML\n",
        "\n",
        "Let's prepare our data for AutoML by splitting it into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_data",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df.drop('price_thousands', axis=1)\n",
        "y = df['price_thousands']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Further split training data for AutoML validation\n",
        "X_train_automl, X_val_automl, y_train_automl, y_val_automl = train_test_split(\n",
        "    X_train, y_train, \n",
        "    test_size=0.25, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"üìä DATA SPLIT SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üéØ Total dataset: {len(df):,} samples\")\n",
        "print(f\"üèãÔ∏è Training set: {len(X_train_automl):,} samples ({len(X_train_automl)/len(df)*100:.1f}%)\")\n",
        "print(f\"‚úÖ Validation set: {len(X_val_automl):,} samples ({len(X_val_automl)/len(df)*100:.1f}%)\")\n",
        "print(f\"üß™ Test set: {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìà FEATURE INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "print(f\"Feature names: {list(X_train.columns)}\")\n",
        "\n",
        "print(f\"\\nüí∞ TARGET VARIABLE STATS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training target range: ${y_train_automl.min():.1f}k - ${y_train_automl.max():.1f}k\")\n",
        "print(f\"Training target mean: ${y_train_automl.mean():.1f}k\")\n",
        "print(f\"Training target std: ${y_train_automl.std():.1f}k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automl_section",
        "colab_type": "text"
      },
      "source": [
        "## ü§ñ Step 5: AutoML with auto-sklearn\n",
        "\n",
        "**auto-sklearn** is an automated machine learning toolkit built on top of scikit-learn. It automatically finds the best algorithm and hyperparameters for your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autosklearn_training",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Configure and train auto-sklearn\n",
        "print(\"üöÄ Starting auto-sklearn training...\")\n",
        "print(\"‚è∞ This may take 5-10 minutes in Colab\")\n",
        "\n",
        "# Create auto-sklearn regressor\n",
        "automl_sklearn = autosklearn.regression.AutoSklearnRegressor(\n",
        "    time_left_for_this_task=300,  # 5 minutes total\n",
        "    per_run_time_limit=30,        # 30 seconds per model\n",
        "    n_jobs=1,                     # Use single core in Colab\n",
        "    memory_limit=3072,            # 3GB memory limit\n",
        "    seed=42,\n",
        "    metric=autosklearn.metrics.mean_squared_error,\n",
        "    resampling_strategy='cv',     # Cross-validation\n",
        "    resampling_strategy_arguments={'folds': 3}\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = datetime.now()\n",
        "automl_sklearn.fit(X_train_automl, y_train_automl)\n",
        "training_time = datetime.now() - start_time\n",
        "\n",
        "print(f\"‚úÖ auto-sklearn training completed in {training_time}\")\n",
        "print(f\"üéØ Models evaluated: {len(automl_sklearn.leaderboard())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autosklearn_results",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Evaluate auto-sklearn performance\n",
        "print(\"üìä AUTO-SKLEARN RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sklearn_val = automl_sklearn.predict(X_val_automl)\n",
        "y_pred_sklearn_test = automl_sklearn.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_automl, y_pred_sklearn_val))\n",
        "val_mae = mean_absolute_error(y_val_automl, y_pred_sklearn_val)\n",
        "val_r2 = r2_score(y_val_automl, y_pred_sklearn_val)\n",
        "\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_sklearn_test))\n",
        "test_mae = mean_absolute_error(y_test, y_pred_sklearn_test)\n",
        "test_r2 = r2_score(y_test, y_pred_sklearn_test)\n",
        "\n",
        "print(f\"üìà Validation Performance:\")\n",
        "print(f\"   RMSE: ${val_rmse:.2f}k\")\n",
        "print(f\"   MAE:  ${val_mae:.2f}k\")\n",
        "print(f\"   R¬≤:   {val_r2:.3f}\")\n",
        "\n",
        "print(f\"\\nüß™ Test Performance:\")\n",
        "print(f\"   RMSE: ${test_rmse:.2f}k\")\n",
        "print(f\"   MAE:  ${test_mae:.2f}k\")\n",
        "print(f\"   R¬≤:   {test_r2:.3f}\")\n",
        "\n",
        "# Show model statistics\n",
        "print(f\"\\nüèÜ MODEL LEADERBOARD\")\n",
        "print(\"=\" * 50)\n",
        "leaderboard = automl_sklearn.leaderboard()\n",
        "print(leaderboard.head())\n",
        "\n",
        "# Show best models\n",
        "print(f\"\\nü•á BEST MODELS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(automl_sklearn.sprint_statistics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpot_section",
        "colab_type": "text"
      },
      "source": [
        "## üß¨ Step 6: AutoML with TPOT\n",
        "\n",
        "**TPOT** (Tree-based Pipeline Optimization Tool) uses genetic programming to automatically design and optimize machine learning pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpot_training",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Configure and train TPOT\n",
        "print(\"üß¨ Starting TPOT training...\")\n",
        "print(\"‚è∞ This may take 3-5 minutes in Colab\")\n",
        "\n",
        "# Create TPOT regressor\n",
        "automl_tpot = TPOTRegressor(\n",
        "    generations=5,           # Number of iterations\n",
        "    population_size=20,      # Number of individuals per generation\n",
        "    cv=3,                    # Cross-validation folds\n",
        "    scoring='neg_mean_squared_error',\n",
        "    max_time_mins=3,         # Maximum time in minutes\n",
        "    max_eval_time_mins=0.5,  # Maximum time per pipeline\n",
        "    random_state=42,\n",
        "    n_jobs=1,                # Single core for Colab\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "start_time = datetime.now()\n",
        "automl_tpot.fit(X_train_automl, y_train_automl)\n",
        "training_time = datetime.now() - start_time\n",
        "\n",
        "print(f\"‚úÖ TPOT training completed in {training_time}\")\n",
        "print(f\"üèÜ Best pipeline score: {automl_tpot.score(X_val_automl, y_val_automl):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpot_results",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Evaluate TPOT performance\n",
        "print(\"üìä TPOT RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tpot_val = automl_tpot.predict(X_val_automl)\n",
        "y_pred_tpot_test = automl_tpot.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "val_rmse_tpot = np.sqrt(mean_squared_error(y_val_automl, y_pred_tpot_val))\n",
        "val_mae_tpot = mean_absolute_error(y_val_automl, y_pred_tpot_val)\n",
        "val_r2_tpot = r2_score(y_val_automl, y_pred_tpot_val)\n",
        "\n",
        "test_rmse_tpot = np.sqrt(mean_squared_error(y_test, y_pred_tpot_test))\n",
        "test_mae_tpot = mean_absolute_error(y_test, y_pred_tpot_test)\n",
        "test_r2_tpot = r2_score(y_test, y_pred_tpot_test)\n",
        "\n",
        "print(f\"üìà Validation Performance:\")\n",
        "print(f\"   RMSE: ${val_rmse_tpot:.2f}k\")\n",
        "print(f\"   MAE:  ${val_mae_tpot:.2f}k\")\n",
        "print(f\"   R¬≤:   {val_r2_tpot:.3f}\")\n",
        "\n",
        "print(f\"\\nüß™ Test Performance:\")\n",
        "print(f\"   RMSE: ${test_rmse_tpot:.2f}k\")\n",
        "print(f\"   MAE:  ${test_mae_tpot:.2f}k\")\n",
        "print(f\"   R¬≤:   {test_r2_tpot:.3f}\")\n",
        "\n",
        "# Show the best pipeline\n",
        "print(f\"\\nüèÜ BEST PIPELINE DISCOVERED\")\n",
        "print(\"=\" * 50)\n",
        "print(automl_tpot.fitted_pipeline_)\n",
        "\n",
        "# Export the pipeline code\n",
        "print(f\"\\nüíæ Exporting optimized pipeline code...\")\n",
        "automl_tpot.export('tpot_best_pipeline.py')\n",
        "print(\"‚úÖ Pipeline exported as 'tpot_best_pipeline.py'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline_comparison",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 7: Baseline Comparison\n",
        "\n",
        "Let's compare our AutoML results with traditional machine learning models to see the benefit of automation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baseline_models",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Train baseline models for comparison\n",
        "print(\"üèÅ Training baseline models for comparison...\")\n",
        "\n",
        "# Scale features for linear regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_automl)\n",
        "X_val_scaled = scaler.transform(X_val_automl)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define baseline models\n",
        "baseline_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "baseline_results = {}\n",
        "\n",
        "for name, model in baseline_models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    # Use scaled data for linear regression, original for tree-based\n",
        "    if name == 'Linear Regression':\n",
        "        model.fit(X_train_scaled, y_train_automl)\n",
        "        y_pred_val = model.predict(X_val_scaled)\n",
        "        y_pred_test = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train_automl, y_train_automl)\n",
        "        y_pred_val = model.predict(X_val_automl)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    baseline_results[name] = {\n",
        "        'val_rmse': np.sqrt(mean_squared_error(y_val_automl, y_pred_val)),\n",
        "        'val_mae': mean_absolute_error(y_val_automl, y_pred_val),\n",
        "        'val_r2': r2_score(y_val_automl, y_pred_val),\n",
        "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
        "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
        "        'test_r2': r2_score(y_test, y_pred_test),\n",
        "        'predictions_test': y_pred_test\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Baseline models trained successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_comparison",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "print(\"üèÜ COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Compile all results\n",
        "all_results = {\n",
        "    'auto-sklearn': {\n",
        "        'val_rmse': val_rmse, 'val_mae': val_mae, 'val_r2': val_r2,\n",
        "        'test_rmse': test_rmse, 'test_mae': test_mae, 'test_r2': test_r2,\n",
        "        'predictions_test': y_pred_sklearn_test\n",
        "    },\n",
        "    'TPOT': {\n",
        "        'val_rmse': val_rmse_tpot, 'val_mae': val_mae_tpot, 'val_r2': val_r2_tpot,\n",
        "        'test_rmse': test_rmse_tpot, 'test_mae': test_mae_tpot, 'test_r2': test_r2_tpot,\n",
        "        'predictions_test': y_pred_tpot_test\n",
        "    }\n",
        "}\n",
        "all_results.update(baseline_results)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(all_results.keys()),\n",
        "    'Test_RMSE': [all_results[model]['test_rmse'] for model in all_results.keys()],\n",
        "    'Test_MAE': [all_results[model]['test_mae'] for model in all_results.keys()],\n",
        "    'Test_R2': [all_results[model]['test_r2'] for model in all_results.keys()],\n",
        "    'Val_RMSE': [all_results[model]['val_rmse'] for model in all_results.keys()],\n",
        "    'Val_R2': [all_results[model]['val_r2'] for model in all_results.keys()]\n",
        "})\n",
        "\n",
        "# Sort by test R¬≤ score\n",
        "comparison_df = comparison_df.sort_values('Test_R2', ascending=False)\n",
        "\n",
        "print(\"üìä PERFORMANCE RANKINGS (by Test R¬≤)\")\n",
        "print(\"=\" * 80)\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    rank = comparison_df.index.get_loc(idx) + 1\n",
        "    print(f\"{rank}. {row['Model']:15} | R¬≤: {row['Test_R2']:.3f} | RMSE: ${row['Test_RMSE']:.1f}k | MAE: ${row['Test_MAE']:.1f}k\")\n",
        "\n",
        "# Find best model\n",
        "best_model = comparison_df.iloc[0]['Model']\n",
        "print(f\"\\nü•á WINNER: {best_model}\")\n",
        "print(f\"üéØ Best Test R¬≤: {comparison_df.iloc[0]['Test_R2']:.3f}\")\n",
        "print(f\"üìâ Best Test RMSE: ${comparison_df.iloc[0]['Test_RMSE']:.1f}k\")\n",
        "\n",
        "# Display the comparison table\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualizations",
        "colab_type": "text"
      },
      "source": [
        "## üìä Step 8: Advanced Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations to understand model performance and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_plots",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Interactive model performance comparison\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Model Performance Comparison (R¬≤)\",\n",
        "        \"RMSE Comparison\",\n",
        "        \"Actual vs Predicted (Best Model)\",\n",
        "        \"Residuals Analysis (Best Model)\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Performance bar charts\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "\n",
        "# R¬≤ comparison\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=comparison_df['Model'], \n",
        "        y=comparison_df['Test_R2'],\n",
        "        name=\"Test R¬≤\",\n",
        "        marker_color=colors\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# RMSE comparison\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=comparison_df['Model'], \n",
        "        y=comparison_df['Test_RMSE'],\n",
        "        name=\"Test RMSE\",\n",
        "        marker_color=colors\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Best model predictions\n",
        "best_predictions = all_results[best_model]['predictions_test']\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y_test, \n",
        "        y=best_predictions,\n",
        "        mode='markers',\n",
        "        name=f\"{best_model} Predictions\",\n",
        "        opacity=0.7\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val, max_val = y_test.min(), y_test.max()\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=[min_val, max_val], \n",
        "        y=[min_val, max_val],\n",
        "        mode='lines',\n",
        "        name=\"Perfect Prediction\",\n",
        "        line=dict(dash='dash', color='red')\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Residuals plot\n",
        "residuals = y_test - best_predictions\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=best_predictions, \n",
        "        y=residuals,\n",
        "        mode='markers',\n",
        "        name=\"Residuals\",\n",
        "        opacity=0.7\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Zero line for residuals\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title_text=f\"üéØ Model Performance Dashboard - Winner: {best_model}\",\n",
        "    title_x=0.5,\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prediction_analysis",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Detailed prediction analysis for the best model\n",
        "print(f\"üîç DETAILED ANALYSIS: {best_model}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate prediction accuracy ranges\n",
        "residuals = y_test - best_predictions\n",
        "percentage_errors = (residuals / y_test) * 100\n",
        "\n",
        "# Accuracy metrics\n",
        "within_5_percent = np.abs(percentage_errors) <= 5\n",
        "within_10_percent = np.abs(percentage_errors) <= 10\n",
        "within_15_percent = np.abs(percentage_errors) <= 15\n",
        "\n",
        "print(f\"üìä PREDICTION ACCURACY\")\n",
        "print(f\"   Within 5%:  {within_5_percent.mean()*100:.1f}% of predictions\")\n",
        "print(f\"   Within 10%: {within_10_percent.mean()*100:.1f}% of predictions\")\n",
        "print(f\"   Within 15%: {within_15_percent.mean()*100:.1f}% of predictions\")\n",
        "\n",
        "print(f\"\\nüìè ERROR STATISTICS\")\n",
        "print(f\"   Mean Error: ${residuals.mean():.2f}k\")\n",
        "print(f\"   Std Error:  ${residuals.std():.2f}k\")\n",
        "print(f\"   Max Over-prediction:  ${residuals.max():.2f}k\")\n",
        "print(f\"   Max Under-prediction: ${residuals.min():.2f}k\")\n",
        "\n",
        "# Business impact analysis\n",
        "print(f\"\\nüíº BUSINESS IMPACT\")\n",
        "print(f\"   Average house price: ${y_test.mean():.0f}k\")\n",
        "print(f\"   RMSE as % of avg price: {(all_results[best_model]['test_rmse']/y_test.mean())*100:.1f}%\")\n",
        "print(f\"   MAE as % of avg price:  {(all_results[best_model]['test_mae']/y_test.mean())*100:.1f}%\")\n",
        "\n",
        "# Sample predictions\n",
        "print(f\"\\nüè† SAMPLE PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "sample_indices = np.random.choice(len(y_test), 5, replace=False)\n",
        "for i, idx in enumerate(sample_indices[:5]):\n",
        "    actual = y_test.iloc[idx]\n",
        "    predicted = best_predictions[idx]\n",
        "    error = predicted - actual\n",
        "    error_pct = (error / actual) * 100\n",
        "    print(f\"House {i+1}: Actual=${actual:.0f}k, Predicted=${predicted:.0f}k, Error={error:+.1f}k ({error_pct:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insights_section",
        "colab_type": "text"
      },
      "source": [
        "## üéì Step 9: Key Insights and Learning\n",
        "\n",
        "Let's summarize what we've learned about AutoML for regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_insights",
        "colab_type": "code"
      },
      "outputs": [],
      "source": [
        "# Generate comprehensive insights\n",
        "print(\"üéì KEY LEARNING INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"ü§ñ AUTOML BENEFITS DEMONSTRATED:\")\n",
        "print(f\"   ‚Ä¢ {best_model} achieved the best performance with R¬≤ = {comparison_df.iloc[0]['Test_R2']:.3f}\")\n",
        "print(f\"   ‚Ä¢ AutoML models outperformed simple baselines\")\n",
        "print(f\"   ‚Ä¢ Automated feature engineering and hyperparameter tuning\")\n",
        "print(f\"   ‚Ä¢ No manual algorithm selection required\")\n",
        "\n",
        "print(f\"\\nüìä REGRESSION METRICS EXPLAINED:\")\n",
        "print(f\"   ‚Ä¢ R¬≤ (Coefficient of Determination): {comparison_df.iloc[0]['Test_R2']:.3f}\")\n",
        "print(f\"     ‚Üí Explains {comparison_df.iloc[0]['Test_R2']*100:.1f}% of price variance\")\n",
        "print(f\"   ‚Ä¢ RMSE (Root Mean Squared Error): ${comparison_df.iloc[0]['Test_RMSE']:.1f}k\")\n",
        "print(f\"     ‚Üí Average prediction error magnitude\")\n",
        "print(f\"   ‚Ä¢ MAE (Mean Absolute Error): ${comparison_df.iloc[0]['Test_MAE']:.1f}k\")\n",
        "print(f\"     ‚Üí Median prediction error\")\n",
        "\n",
        "print(f\"\\nüèÜ MODEL COMPARISON INSIGHTS:\")\n",
        "automl_models = ['auto-sklearn', 'TPOT']\n",
        "baseline_models_list = ['Linear Regression', 'Random Forest']\n",
        "\n",
        "best_automl = comparison_df[comparison_df['Model'].isin(automl_models)].iloc[0]\n",
        "best_baseline = comparison_df[comparison_df['Model'].isin(baseline_models_list)].iloc[0]\n",
        "\n",
        "improvement = ((best_automl['Test_R2'] - best_baseline['Test_R2']) / best_baseline['Test_R2']) * 100\n",
        "print(f\"   ‚Ä¢ Best AutoML: {best_automl['Model']} (R¬≤ = {best_automl['Test_R2']:.3f})\")\n",
        "print(f\"   ‚Ä¢ Best Baseline: {best_baseline['Model']} (R¬≤ = {best_baseline['Test_R2']:.3f})\")\n",
        "print(f\"   ‚Ä¢ AutoML improvement: {improvement:+.1f}% better R¬≤ score\")\n",
        "\n",
        "print(f\"\\nüéØ PRACTICAL APPLICATIONS:\")\n",
        "print(f\"   ‚Ä¢ Real Estate: Automated property valuation\")\n",
        "print(f\"   ‚Ä¢ Finance: Credit scoring and risk assessment\")\n",
        "print(f\"   ‚Ä¢ Manufacturing: Quality control and defect prediction\")\n",
        "print(f\"   ‚Ä¢ Healthcare: Treatment outcome prediction\")\n",
        "print(f\"   ‚Ä¢ Marketing: Customer lifetime value estimation\")\n",
        "\n",
        "print(f\"\\nüí° NEXT STEPS FOR PRODUCTION:\")\n",
        "print(f\"   ‚Ä¢ Feature engineering: Create domain-specific features\")\n",
        "print(f\"   ‚Ä¢ Data quality: Handle missing values and outliers\")\n",
        "print(f\"   ‚Ä¢ Model monitoring: Track performance over time\")\n",
        "print(f\"   ‚Ä¢ A/B testing: Compare models in production\")\n",
        "print(f\"   ‚Ä¢ Explainability: Use SHAP for model interpretability\")\n",
        "\n",
        "print(f\"\\nüîó AUTOML LIBRARIES COMPARISON:\")\n",
        "sklearn_r2 = all_results['auto-sklearn']['test_r2']\n",
        "tpot_r2 = all_results['TPOT']['test_r2']\n",
        "print(f\"   ‚Ä¢ auto-sklearn: R¬≤ = {sklearn_r2:.3f} | Focus: Robust, ensemble methods\")\n",
        "print(f\"   ‚Ä¢ TPOT: R¬≤ = {tpot_r2:.3f} | Focus: Genetic programming, pipeline optimization\")\n",
        "print(f\"   ‚Ä¢ Both excel at different aspects of AutoML\")\n",
        "\n",
        "print(f\"\\nüéä CONGRATULATIONS!\")\n",
        "print(f\"You've successfully implemented AutoML for regression and achieved:\")\n",
        "print(f\"üèÜ Best Model: {best_model}\")\n",
        "print(f\"üìà R¬≤ Score: {comparison_df.iloc[0]['Test_R2']:.3f}\")\n",
        "print(f\"üí∞ Average Error: ${comparison_df.iloc[0]['Test_MAE']:.1f}k\")\n",
        "print(f\"‚ú® {within_10_percent.mean()*100:.1f}% of predictions within 10% accuracy!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion",
        "colab_type": "text"
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "In this notebook, we successfully:\n",
        "\n",
        "1. **üèóÔ∏è Created a realistic dataset** with 15 features for house price prediction\n",
        "2. **üìä Performed comprehensive EDA** to understand data relationships\n",
        "3. **ü§ñ Implemented two AutoML approaches**: auto-sklearn and TPOT\n",
        "4. **üìà Compared AutoML vs traditional models** and demonstrated improvements\n",
        "5. **üîç Analyzed predictions** with detailed performance metrics\n",
        "6. **üìä Created interactive visualizations** for better insights\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **AutoML democratizes machine learning** by automating complex tasks\n",
        "- **Different AutoML tools excel in different scenarios** - experiment with multiple approaches\n",
        "- **Evaluation metrics matter** - R¬≤, RMSE, and MAE each tell different stories\n",
        "- **Visualization is crucial** for understanding model behavior\n",
        "- **Real-world applications are vast** - from real estate to healthcare\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try with your own data**: Upload a CSV and adapt this notebook\n",
        "2. **Experiment with hyperparameters**: Adjust time limits and population sizes\n",
        "3. **Add feature engineering**: Create polynomial features or domain-specific transformations\n",
        "4. **Explore model interpretability**: Use SHAP or LIME for explainable AI\n",
        "5. **Deploy your model**: Create a web app or API endpoint\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Happy AutoML Learning!**\n",
        "\n",
        "Feel free to modify this notebook for your own regression problems. AutoML makes machine learning accessible to everyone!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}